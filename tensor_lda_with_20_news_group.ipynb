{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Topic extraction with Tensor LDA\n",
    "\n",
    "\n",
    "This example is modified from scikit-learn's \"Topic extraction with\n",
    "Non-negative Matrix Factorization and Latent Dirichlet Allocation\"\n",
    "example.\n",
    "\n",
    "This example applies :class:`tensor_lda.tensor_lda.TensorLDA`\n",
    "on the 20 news group dataset and the output is a list of topics, each\n",
    "represented as a list of terms (weights are not shown).\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "done in 1.131s.\n",
      "Extracting tf features for LDA...\n",
      "done in 1.175s.\n",
      "\n",
      "Fitting TensorLDA models with tf features, n_samples=10000 and n_features=1000...\n",
      "done in 42.296s.\n",
      "\n",
      "Topics in LDA model:\n",
      "Topic #0 (prior: 0.000): men women 1993 oh april say said child life thing\n",
      "Topic #1 (prior: 0.000): ax max b8f g9v pl bhj giz wm bxn sl\n",
      "Topic #2 (prior: 0.000): 00 new sale 01 thank 20 50 30 shipping dos\n",
      "Topic #3 (prior: 0.000): window use way windows place card program problem display does\n",
      "Topic #4 (prior: 0.000): groups new group need israel doesn program windows jewish mail\n",
      "Topic #5 (prior: 0.000): com try list article said dave think david source sun\n",
      "Topic #6 (prior: 0.000): edu article soon israel university probably com news question uk\n",
      "Topic #7 (prior: 0.001): address does send mail email ve thanks post looking uk\n",
      "Topic #8 (prior: 0.001): car need like really cars looks worth makes guess end\n",
      "Topic #9 (prior: 0.001): key keys chip use public number know bit used using\n",
      "Topic #10 (prior: 0.001): drive scsi hard disk drives rom floppy ide 25 possible\n",
      "Topic #11 (prior: 0.001): ve got like best seen hope right good don just\n",
      "Topic #12 (prior: 0.001): card video does know thanks need monitor windows driver drivers\n",
      "Topic #13 (prior: 0.001): israel question israeli article jews war state jewish say subject\n",
      "Topic #14 (prior: 0.001): game team games year win does know think play hockey\n",
      "Topic #15 (prior: 0.001): windows file files thanks know program ftp does available use\n",
      "Topic #16 (prior: 0.002): god edu right soon believe jesus does article university word\n",
      "Topic #17 (prior: 0.002): year way did didn years work thanks going just different\n",
      "Topic #18 (prior: 0.002): 10 11 12 like 1993 15 20 24 april 00\n",
      "Topic #19 (prior: 0.002): new sale 00 good price used shipping like asking offer\n",
      "Topic #20 (prior: 0.002): problem think try file don new just work ve problems\n",
      "Topic #21 (prior: 0.003): like don way use got sounds source look file right\n",
      "Topic #22 (prior: 0.003): think good team don new just time ve win stuff\n",
      "Topic #23 (prior: 0.003): people think way year don team did years make good\n",
      "Topic #24 (prior: 0.003): does read different say number true did work try people\n",
      "Topic #25 (prior: 0.004): just know people wanted thought ll let wasn thanks ve\n",
      "Topic #26 (prior: 0.002): god jesus believe people faith does word make say time\n",
      "Topic #27 (prior: 0.002): right got new just don state stuff said started time\n",
      "Topic #28 (prior: 0.003): 00 need 01 use help thank really think wrong information\n",
      "Topic #29 (prior: 0.004): edu soon article university like just thanks don unit internet\n",
      "Topic #30 (prior: 0.004): available program software ftp files windows public pc does edu\n",
      "Topic #31 (prior: 0.005): use window using want think used need windows sure program\n",
      "Topic #32 (prior: 0.005): new york stuff years just year groups know did old\n",
      "Topic #33 (prior: 0.003): good sale asking make offer want thanks best looking email\n",
      "Topic #34 (prior: 0.006): need work windows don want driver know just use ve\n",
      "Topic #35 (prior: 0.006): thanks just problem way going power use year make want\n",
      "Topic #36 (prior: 0.006): don know people just time think like said did really\n",
      "Topic #37 (prior: 0.006): know time mean don israel soon key israeli war wasn\n",
      "Topic #38 (prior: 0.005): does know thanks like mail looking windows help list need\n",
      "Topic #39 (prior: 0.005): does let soon doesn best article used sale team edu\n",
      "\n",
      "[1.03685825e-06 2.86157411e-06 9.59344697e-06 1.22371362e-05\n",
      " 3.04914554e-05 3.06031168e-05 3.34866967e-05 5.55072151e-05\n",
      " 8.38389944e-05 6.13189134e-05 7.39053605e-05 7.63742309e-05\n",
      " 8.35074454e-05 1.00970653e-04 1.04156641e-04 1.09855151e-04\n",
      " 1.17620869e-04 9.92523334e-01 1.72182677e-04 1.69469914e-04\n",
      " 1.80259679e-04 2.64751480e-04 2.41726206e-04 2.38491020e-04\n",
      " 2.49051523e-04 2.87753720e-04 1.41419518e-04 1.83756844e-04\n",
      " 2.02733144e-04 2.93520835e-04 2.95317176e-04 3.70080629e-04\n",
      " 3.52056193e-04 2.55048915e-04 4.29835143e-04 4.30097121e-04\n",
      " 4.72828360e-04 4.25679879e-04 4.17363851e-04 4.15876694e-04]\n",
      "Something about how Koresh had threatened to cause local \n",
      "problems with all these wepaons he had and was alleged to\n",
      "have.  \n",
      "\n",
      "Someone else will post more details soon, I'm sure.\n",
      "\n",
      "Other News:\n",
      "Sniper injures 9 outside MCA buildling in L.A.  Man arrested--suspect\n",
      "was disgruntled employee of Universal Studios, which\n",
      "is a division of M.C.A.\n",
      "\n",
      "\n",
      "QUESTION:\n",
      "What will Californians do with all those guns after the Reginald\n",
      "denny trial?\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from time import time\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "from tensor_lda.tensor_lda import TensorLDA\n",
    "\n",
    "n_samples = 10000\n",
    "n_features = 1000\n",
    "n_components = 40\n",
    "n_top_words = 10\n",
    "\n",
    "\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        topic_prior = model.alpha_[topic_idx]\n",
    "        message = \"Topic #%d (prior: %.3f): \" % (topic_idx, topic_prior)\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()\n",
    "\n",
    "\n",
    "# Load the 20 newsgroups dataset and vectorize it. We use a few heuristics\n",
    "# to filter out useless terms early on: the posts are stripped of headers,\n",
    "# footers and quoted replies, and common English words, words occurring in\n",
    "# only one document or in at least 95% of the documents are removed.\n",
    "\n",
    "print(\"Loading dataset...\")\n",
    "t0 = time()\n",
    "dataset = fetch_20newsgroups(shuffle=True, random_state=2,\n",
    "                             remove=('headers', 'footers', 'quotes'))\n",
    "data_samples = dataset.data[:n_samples]\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "# Use tf (raw term count) features for LDA.\n",
    "print(\"Extracting tf features for LDA...\")\n",
    "tf_vectorizer = CountVectorizer(max_df=0.8, min_df=5,\n",
    "                                max_features=n_features,\n",
    "                                stop_words='english')\n",
    "t0 = time()\n",
    "tf = tf_vectorizer.fit_transform(data_samples)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "print()\n",
    "\n",
    "print(\"Fitting TensorLDA models with tf features, \"\n",
    "      \"n_samples=%d and n_features=%d...\"\n",
    "      % (n_samples, n_features))\n",
    "\n",
    "lda = TensorLDA(n_components=n_components, alpha0=.1)\n",
    "\n",
    "t0 = time()\n",
    "lda.fit(tf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "print(\"\\nTopics in LDA model:\")\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "print_top_words(lda, tf_feature_names, n_top_words)\n",
    "\n",
    "doc_topics = lda.transform(tf[0:2, :])\n",
    "print(doc_topics[0, :])\n",
    "print(data_samples[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<10000x1000 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 254723 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "426"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset['data'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
